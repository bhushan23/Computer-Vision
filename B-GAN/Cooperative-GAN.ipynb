{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as tutils\n",
    "import copy\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "imgDim = 28\n",
    "path = './genImg/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = datasets.MNIST(root='../Data/MNIST',\n",
    "                         train=True,\n",
    "                         transform=transform,\n",
    "                         download=True)\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    IS_CUDA = True\n",
    "    \n",
    "def var(x):\n",
    "    if IS_CUDA:\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.data.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "def save_image(pic, path):\n",
    "    grid = torchvision.utils.make_grid(pic.data, nrow=8, padding=2)\n",
    "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator_input = 64\n",
    "'''\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.resize(batch_size, 1, imgDim, imgDim)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x),2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training = self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.sigmoid(x)\n",
    "'''\n",
    "    \n",
    "    \n",
    "D = nn.Sequential(\n",
    "        nn.Linear(784, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256,1),\n",
    "        nn.Sigmoid())\n",
    "\n",
    "Generator_input = 64\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(Generator_input, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Sequential(nn.Linear(256, 784), nn.Tanh())\n",
    "        self.lR = nn.LeakyReLU(0.2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lR(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lR(x)\n",
    "        return self.fc3(x)\n",
    "'''\n",
    "\n",
    "Generator = nn.Sequential(\n",
    "        nn.Linear(Generator_input, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.LeakyReLU(0.2),\n",
    "        nn.Linear(256, 784),\n",
    "        nn.Tanh())\n",
    "\n",
    "'''\n",
    "#D = Discriminator()\n",
    "\n",
    "# Create n Generators\n",
    "#Gen = copy.deepcopy(Generator)\n",
    "Gen = Generator()\n",
    "Generators = []\n",
    "GOptimizers = []\n",
    "NumberOfGenerators = 3\n",
    "for i in range(NumberOfGenerators):\n",
    "    #Generators.append(copy.deepcopy(Generator))\n",
    "    Generators.append(Generator())\n",
    "\n",
    "if IS_CUDA:\n",
    "    D.cuda()\n",
    "    for each in Generators:\n",
    "        each.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossCriterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr = 0.0001)\n",
    "G_opt = torch.optim.Adam(Gen.parameters(), lr = 0.0001)\n",
    "GOptimizers = []\n",
    "for each in Generators:\n",
    "    GOptimizers.append(torch.optim.Adam(each.parameters(), lr = 0.0001))\n",
    "\n",
    "fixed_x = var(torch.randn(batch_size, Generator_input))\n",
    "\n",
    "GeneratorLoss = []\n",
    "def backPropGenerator(index, GeneratorLoss):\n",
    "    Generators[index].zero_grad()\n",
    "    GeneratorLoss[index].backward()\n",
    "    GOptimizers[index].step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noutputImages = []\\nBestPerformingGenerator = 0\\ndef train(Gen, BestPerformingGenerator, num_epochs = 10, d_iter = 1):\\n    for epoch in range(num_epochs):\\n        for data in data_loader:\\n            image, _  = data\\n            image = var(image.view(image.size(0),  -1))\\n            \\n            #Gen = copy.deepcopy(Generators[BestPerformingGenerator])\\n            \\n            # Train Discriminator\\n            #for k in range(0, d_iter):\\n            # For Log D(x)\\n            #print image.shape\\n            D_real = D(image)\\n            # For Log(1 - D(G(Z)))\\n            Z_noise = var(torch.randn(batch_size, Generator_input))\\n            #print Z_noise.shape\\n            #print type(Gen)\\n            G_fake = Gen(Z_noise)\\n            #print G_fake.shape\\n            D_fake = D(G_fake)\\n\\n            # Calculate Discriminator Loss\\n            D_real_loss = lossCriterion(D_real, var(torch.ones(batch_size, 1)))\\n            D_fake_loss = lossCriterion(D_fake, var(torch.zeros(batch_size, 1)))\\n            D_loss = D_real_loss + D_fake_loss\\n\\n            # Backprop Discriminator\\n            D.zero_grad()\\n            D_loss.backward()\\n            D_opt.step()\\n   \\n            # Train Generators\\n            Z_noise = var(torch.randn(batch_size, Generator_input))\\n            G_fake = Gen(Z_noise)\\n            D_fake = D(G_fake)\\n            # Compute Generator Loss\\n            G_loss = lossCriterion(D_fake, var(torch.ones(batch_size, 1)))\\n            \\n            # Find best performing Generator\\n            GeneratorLoss = []\\n            lossList = []\\n            for each in Generators:\\n                Z_noise1 = var(torch.randn(batch_size, Generator_input))\\n                G_fake1 = each(Z_noise1)\\n                #print G_fake1.shape\\n                #print type(each)\\n                D_fake1 = D(G_fake1)\\n                # Compute Generator Loss\\n                G_loss1 = lossCriterion(D_fake1, var(torch.ones(batch_size, 1)))\\n                GeneratorLoss.append(G_loss1)\\n                lossList.append(float(G_loss1.data[0]))\\n            \\n            #print lossList\\n            #print type(lossList[0])\\n            BestPerformingGenerator = lossList.index(min(lossList))\\n            \\n            # Change other Generator with best performing ones config:\\n            Gen = copy.deepcopy(Generators[BestPerformingGenerator])\\n            G_opt = copy.deepcopy(GOptimizers[BestPerformingGenerator])\\n\\n            # Backprop Genearator\\n            D.zero_grad()\\n            Gen.zero_grad()\\n            G_loss.backward()\\n            G_opt.step()\\n            \\n            backPropGenerator(BestPerformingGenerator, GeneratorLoss)\\n            \\n            for i in range(NumberOfGenerators):\\n                if i != BestPerformingGenerator:\\n                    Generators[i] = copy.deepcopy(Generators[BestPerformingGenerator])\\n                    GOptimizers[i] = copy.deepcopy(GOptimizers[BestPerformingGenerator])\\n                    backPropGenerator(i, GeneratorLoss)\\n                                \\n        #print epoch\\n        print 'Epoch [{}/{}], Discriminator {:.4f}, Generator {:.4f}'.format(epoch+1, num_epochs, D_loss.data[0], G_loss.data[0])\\n        pic = Gen(var(torch.randn(batch_size, Generator_input))) #(fixed_x)\\n        pic = pic.view(pic.size(0), 1, imgDim, imgDim) \\n        outputImages.append(pic)\\n        torchvision.utils.save_image(pic.data, path+'image_{}.png'.format(epoch)) \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "outputImages = []\n",
    "BestPerformingGenerator = 0\n",
    "def train(Gen, BestPerformingGenerator, num_epochs = 10, d_iter = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in data_loader:\n",
    "            image, _  = data\n",
    "            image = var(image.view(image.size(0),  -1))\n",
    "            \n",
    "            #Gen = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "            \n",
    "            # Train Discriminator\n",
    "            #for k in range(0, d_iter):\n",
    "            # For Log D(x)\n",
    "            #print image.shape\n",
    "            D_real = D(image)\n",
    "            # For Log(1 - D(G(Z)))\n",
    "            Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "            #print Z_noise.shape\n",
    "            #print type(Gen)\n",
    "            G_fake = Gen(Z_noise)\n",
    "            #print G_fake.shape\n",
    "            D_fake = D(G_fake)\n",
    "\n",
    "            # Calculate Discriminator Loss\n",
    "            D_real_loss = lossCriterion(D_real, var(torch.ones(batch_size, 1)))\n",
    "            D_fake_loss = lossCriterion(D_fake, var(torch.zeros(batch_size, 1)))\n",
    "            D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "            # Backprop Discriminator\n",
    "            D.zero_grad()\n",
    "            D_loss.backward()\n",
    "            D_opt.step()\n",
    "   \n",
    "            # Train Generators\n",
    "            Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "            G_fake = Gen(Z_noise)\n",
    "            D_fake = D(G_fake)\n",
    "            # Compute Generator Loss\n",
    "            G_loss = lossCriterion(D_fake, var(torch.ones(batch_size, 1)))\n",
    "            \n",
    "            # Find best performing Generator\n",
    "            GeneratorLoss = []\n",
    "            lossList = []\n",
    "            for each in Generators:\n",
    "                Z_noise1 = var(torch.randn(batch_size, Generator_input))\n",
    "                G_fake1 = each(Z_noise1)\n",
    "                #print G_fake1.shape\n",
    "                #print type(each)\n",
    "                D_fake1 = D(G_fake1)\n",
    "                # Compute Generator Loss\n",
    "                G_loss1 = lossCriterion(D_fake1, var(torch.ones(batch_size, 1)))\n",
    "                GeneratorLoss.append(G_loss1)\n",
    "                lossList.append(float(G_loss1.data[0]))\n",
    "            \n",
    "            #print lossList\n",
    "            #print type(lossList[0])\n",
    "            BestPerformingGenerator = lossList.index(min(lossList))\n",
    "            \n",
    "            # Change other Generator with best performing ones config:\n",
    "            Gen = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "            G_opt = copy.deepcopy(GOptimizers[BestPerformingGenerator])\n",
    "\n",
    "            # Backprop Genearator\n",
    "            D.zero_grad()\n",
    "            Gen.zero_grad()\n",
    "            G_loss.backward()\n",
    "            G_opt.step()\n",
    "            \n",
    "            backPropGenerator(BestPerformingGenerator, GeneratorLoss)\n",
    "            \n",
    "            for i in range(NumberOfGenerators):\n",
    "                if i != BestPerformingGenerator:\n",
    "                    Generators[i] = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "                    GOptimizers[i] = copy.deepcopy(GOptimizers[BestPerformingGenerator])\n",
    "                    backPropGenerator(i, GeneratorLoss)\n",
    "                                \n",
    "        #print epoch\n",
    "        print 'Epoch [{}/{}], Discriminator {:.4f}, Generator {:.4f}'.format(epoch+1, num_epochs, D_loss.data[0], G_loss.data[0])\n",
    "        pic = Gen(var(torch.randn(batch_size, Generator_input))) #(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, imgDim, imgDim) \n",
    "        outputImages.append(pic)\n",
    "        torchvision.utils.save_image(pic.data, path+'image_{}.png'.format(epoch)) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#train(Gen, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputImages = []\n",
    "def train(BestPerformingGenerator, num_epochs = 10, d_iter = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        lossList = [0.0] * NumberOfGenerators\n",
    "        for data in data_loader:\n",
    "            image, _  = data\n",
    "            image = var(image.view(image.size(0),  -1))\n",
    "            \n",
    "            #Gen = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "            \n",
    "            # Train Discriminator\n",
    "            #for k in range(0, d_iter):\n",
    "            # For Log D(x)\n",
    "            #print image.shape\n",
    "            #print 'Epoch [{}/{}].'.format(epoch+1, num_epochs)\n",
    "            \n",
    "            #for each in Generators:\n",
    "            D_real = D(image)\n",
    "            # For Log(1 - D(G(Z)))\n",
    "            Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "            #print Z_noise.shape\n",
    "            #print type(Gen)\n",
    "            G_fake = Generators[BestPerformingGenerator](Z_noise)\n",
    "            #print G_fake.shape\n",
    "            D_fake = D(G_fake)\n",
    "\n",
    "            # Calculate Discriminator Loss\n",
    "            D_real_loss = lossCriterion(D_real, var(torch.ones(batch_size, 1)))\n",
    "            D_fake_loss = lossCriterion(D_fake, var(torch.zeros(batch_size, 1)))\n",
    "            D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "            # Backprop Discriminator\n",
    "            D.zero_grad()\n",
    "            D_loss.backward()\n",
    "            D_opt.step()\n",
    "                #print 'Discriminator Loop for: {}: {}'.format(i, D_loss.data[0])\n",
    "   \n",
    "            # Find best performing Generator\n",
    "            i = 0\n",
    "            GeneratorLoss = []\n",
    "            for each, each_opt in zip(Generators, GOptimizers):\n",
    "                Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "                G_fake = each(Z_noise)\n",
    "                #print G_fake1.shape\n",
    "                #print type(each)\n",
    "                D_fake = D(G_fake)\n",
    "                # Compute Generator Loss\n",
    "                G_loss = lossCriterion(D_fake, var(torch.ones(batch_size, 1)))\n",
    "                GeneratorLoss.append(G_loss)\n",
    "                lossList[i] += (float(G_loss.data[0]))\n",
    "                i = i + 1\n",
    "                D.zero_grad()\n",
    "                each.zero_grad()\n",
    "                G_loss.backward()\n",
    "                each_opt.step()\n",
    "                \n",
    "                #backPropGenerator(i, GeneratorLoss)\n",
    "                #print 'Generator Loop for: {}: {}'.format(i, G_loss.data[0])\n",
    "            \n",
    "            #print lossList\n",
    "            #print type(lossList[0])\n",
    "        BestPerformingGenerator = lossList.index(max(lossList)) # earlier was min\n",
    "        print lossList\n",
    "        for i in range(0, NumberOfGenerators):\n",
    "            if i != BestPerformingGenerator:\n",
    "                prev = Generators[i]\n",
    "                Generators[i] = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "                GOptimizers[i] = torch.optim.Adam(Generators[i].parameters(), lr = 0.0001)\n",
    "                #copy.deepcopy(GOptimizers[BestPerformingGenerator])\n",
    "                if Generators[i] == prev:\n",
    "                    print 'SAME'\n",
    "\n",
    "        #print epoch\n",
    "        #print BestPerformingGenerator\n",
    "        print 'Epoch [{}/{}], Discriminator {:.4f}, Best Generator[{}] {:.4f}'.format(epoch+1, num_epochs, D_loss.data[0], BestPerformingGenerator, GeneratorLoss[BestPerformingGenerator].data[0])\n",
    "        pic = Generators[BestPerformingGenerator](var(torch.randn(batch_size, Generator_input))) #(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, imgDim, imgDim) \n",
    "        outputImages.append(pic)\n",
    "        #torchvision.utils.save_image(pic.data, path+'image_{}.png'.format(epoch))   \n",
    "        save_image(pic, path+'image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train D for every G\n",
    "\n",
    "outputImages = []\n",
    "def train(BestPerformingGenerator, num_epochs = 10, d_iter = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        lossList = [0.0] * NumberOfGenerators\n",
    "        for data in data_loader:\n",
    "            image, _  = data\n",
    "            image = var(image.view(image.size(0),  -1))\n",
    "            \n",
    "            #Gen = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "            \n",
    "            # Train Discriminator\n",
    "            #for k in range(0, d_iter):\n",
    "            # For Log D(x)\n",
    "            #print image.shape\n",
    "            #print 'Epoch [{}/{}].'.format(epoch+1, num_epochs)\n",
    "            \n",
    "            for each in Generators:\n",
    "                D_real = D(image)\n",
    "                # For Log(1 - D(G(Z)))\n",
    "                Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "                #print Z_noise.shape\n",
    "                #print type(Gen)\n",
    "                G_fake = each(Z_noise) #Generators[BestPerformingGenerator](Z_noise)\n",
    "                #print G_fake.shape\n",
    "                D_fake = D(G_fake)\n",
    "\n",
    "                # Calculate Discriminator Loss\n",
    "                D_real_loss = lossCriterion(D_real, var(torch.ones(batch_size, 1)))\n",
    "                D_fake_loss = lossCriterion(D_fake, var(torch.zeros(batch_size, 1)))\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "                # Backprop Discriminator\n",
    "                D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                D_opt.step()\n",
    "                #print 'Discriminator Loop for: {}: {}'.format(i, D_loss.data[0])\n",
    "   \n",
    "            # Find best performing Generator\n",
    "            i = 0\n",
    "            GeneratorLoss = []\n",
    "            for each, each_opt in zip(Generators, GOptimizers):\n",
    "                Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "                G_fake = each(Z_noise)\n",
    "                #print G_fake1.shape\n",
    "                #print type(each)\n",
    "                D_fake = D(G_fake)\n",
    "                # Compute Generator Loss\n",
    "                G_loss = lossCriterion(D_fake, var(torch.ones(batch_size, 1)))\n",
    "                GeneratorLoss.append(G_loss)\n",
    "                lossList[i] += (float(G_loss.data[0]))\n",
    "                i = i + 1\n",
    "                D.zero_grad()\n",
    "                each.zero_grad()\n",
    "                G_loss.backward()\n",
    "                each_opt.step()\n",
    "                \n",
    "                #backPropGenerator(i, GeneratorLoss)\n",
    "                #print 'Generator Loop for: {}: {}'.format(i, G_loss.data[0])\n",
    "            \n",
    "            #print lossList\n",
    "            #print type(lossList[0])\n",
    "        BestPerformingGenerator = lossList.index(max(lossList)) # earlier was min\n",
    "        print lossList\n",
    "        for i in range(0, NumberOfGenerators):\n",
    "            if i != BestPerformingGenerator:\n",
    "                prev = Generators[i]\n",
    "                Generators[i] = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "                GOptimizers[i] = torch.optim.Adam(Generators[i].parameters(), lr = 0.0001)\n",
    "                #copy.deepcopy(GOptimizers[BestPerformingGenerator])\n",
    "                if Generators[i] == prev:\n",
    "                    print 'SAME'\n",
    "\n",
    "        #print epoch\n",
    "        #print BestPerformingGenerator\n",
    "        print 'Epoch [{}/{}], Discriminator {:.4f}, Best Generator[{}] {:.4f}'.format(epoch+1, num_epochs, D_loss.data[0], BestPerformingGenerator, GeneratorLoss[BestPerformingGenerator].data[0])\n",
    "        pic = Generators[BestPerformingGenerator](var(torch.randn(batch_size, Generator_input))) #(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, imgDim, imgDim) \n",
    "        outputImages.append(pic)\n",
    "        #torchvision.utils.save_image(pic.data, path+'image_{}.png'.format(epoch))   \n",
    "        save_image(pic, path+'image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model instead of deepcopy\n",
    "\n",
    "outputImages = []\n",
    "def train(BestPerformingGenerator, num_epochs = 10, d_iter = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        lossList = [0.0] * NumberOfGenerators\n",
    "        for data in data_loader:\n",
    "            image, _  = data\n",
    "            image = var(image.view(image.size(0),  -1))\n",
    "            \n",
    "            #Gen = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "            \n",
    "            # Train Discriminator\n",
    "            #for k in range(0, d_iter):\n",
    "            # For Log D(x)\n",
    "            #print image.shape\n",
    "            #print 'Epoch [{}/{}].'.format(epoch+1, num_epochs)\n",
    "            \n",
    "            for each in Generators:\n",
    "                D_real = D(image)\n",
    "                # For Log(1 - D(G(Z)))\n",
    "                Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "                #print Z_noise.shape\n",
    "                #print type(Gen)\n",
    "                G_fake = each(Z_noise) #Generators[BestPerformingGenerator](Z_noise)\n",
    "                #print G_fake.shape\n",
    "                D_fake = D(G_fake)\n",
    "\n",
    "                # Calculate Discriminator Loss\n",
    "                D_real_loss = lossCriterion(D_real, var(torch.ones(batch_size, 1)))\n",
    "                D_fake_loss = lossCriterion(D_fake, var(torch.zeros(batch_size, 1)))\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "                # Backprop Discriminator\n",
    "                D.zero_grad()\n",
    "                D_loss.backward()\n",
    "                D_opt.step()\n",
    "                #print 'Discriminator Loop for: {}: {}'.format(i, D_loss.data[0])\n",
    "   \n",
    "            # Find best performing Generator\n",
    "            i = 0\n",
    "            GeneratorLoss = []\n",
    "            for each, each_opt in zip(Generators, GOptimizers):\n",
    "                Z_noise = var(torch.randn(batch_size, Generator_input))\n",
    "                G_fake = each(Z_noise)\n",
    "                #print G_fake1.shape\n",
    "                #print type(each)\n",
    "                D_fake = D(G_fake)\n",
    "                # Compute Generator Loss\n",
    "                G_loss = lossCriterion(D_fake, var(torch.ones(batch_size, 1)))\n",
    "                GeneratorLoss.append(G_loss)\n",
    "                lossList[i] += (float(G_loss.data[0]))\n",
    "                i = i + 1\n",
    "                D.zero_grad()\n",
    "                each.zero_grad()\n",
    "                G_loss.backward()\n",
    "                each_opt.step()\n",
    "                \n",
    "                #backPropGenerator(i, GeneratorLoss)\n",
    "                #print 'Generator Loop for: {}: {}'.format(i, G_loss.data[0])\n",
    "            \n",
    "            #print lossList\n",
    "            #print type(lossList[0])\n",
    "        BestPerformingGenerator = lossList.index(max(lossList)) # earlier was min\n",
    "        print lossList\n",
    "        for i in range(0, NumberOfGenerators):\n",
    "            if i != BestPerformingGenerator:\n",
    "                prev = Generators[i]\n",
    "                Generators[i] = Generator()\n",
    "                Generators[i].load_state_dict(Generators[BestPerformingGenerator].state_dict())\n",
    "                GOptimizers[i] = torch.optim.Adam(Generators[i].parameters(), lr = 0.0001)\n",
    "                GOptimizers[i].load_state_dict(GOptimizers[BestPerformingGenerator].state_dict())\n",
    "                #Generators[i] = copy.deepcopy(Generators[BestPerformingGenerator])\n",
    "                #GOptimizers[i] = torch.optim.Adam(Generators[i].parameters(), lr = 0.0001)\n",
    "                #copy.deepcopy(GOptimizers[BestPerformingGenerator])\n",
    "                if Generators[i] == prev:\n",
    "                    print 'SAME'\n",
    "\n",
    "        #print epoch\n",
    "        #print BestPerformingGenerator\n",
    "        print 'Epoch [{}/{}], Discriminator {:.4f}, Best Generator[{}] {:.4f}'.format(epoch+1, num_epochs, D_loss.data[0], BestPerformingGenerator, GeneratorLoss[BestPerformingGenerator].data[0])\n",
    "        pic = Generators[BestPerformingGenerator](var(torch.randn(batch_size, Generator_input))) #(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, imgDim, imgDim) \n",
    "        outputImages.append(pic)\n",
    "        #torchvision.utils.save_image(pic.data, path+'image_{}.png'.format(epoch))   \n",
    "        torchvision.utils.save_image(denorm(pic.data), path+'images-%d.png' %(epoch+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2532.735988497734, 2510.1840450763702, 2678.7209434509277]\n",
      "Epoch [1/30], Discriminator 0.0544, Best Generator[2] 4.8398\n",
      "[3708.898733139038, 3699.0548515319824, 3691.1821489334106]\n",
      "Epoch [2/30], Discriminator 0.0139, Best Generator[0] 6.0457\n",
      "[2978.1063261032104, 2990.697278857231, 2994.3041499853134]\n",
      "Epoch [3/30], Discriminator 0.0144, Best Generator[2] 5.9195\n",
      "[2862.3172655701637, 2726.8054524064064, 2686.2068423628807]\n",
      "Epoch [4/30], Discriminator 0.1163, Best Generator[0] 4.1751\n",
      "[3176.44689142704, 3026.2978740036488, 3353.713204741478]\n",
      "Epoch [5/30], Discriminator 0.1805, Best Generator[2] 3.0685\n",
      "[3912.1018545627594, 4083.277430295944, 3572.6396095752716]\n",
      "Epoch [6/30], Discriminator 0.1003, Best Generator[1] 4.9545\n",
      "[4688.705378457904, 4699.2129918932915, 4705.606785058975]\n",
      "Epoch [7/30], Discriminator 0.3109, Best Generator[2] 10.4148\n",
      "[5081.209809780121, 4477.288584530354, 5128.331526756287]\n",
      "Epoch [8/30], Discriminator 0.1063, Best Generator[2] 3.9163\n",
      "[4918.330916404724, 5007.843717813492, 4563.767076849937]\n",
      "Epoch [9/30], Discriminator 0.2933, Best Generator[1] 5.7451\n",
      "[4980.428693681955, 4745.174172639847, 4432.675302028656]\n",
      "Epoch [10/30], Discriminator 0.2153, Best Generator[0] 6.3072\n",
      "[4773.129043072462, 4222.666866183281, 4704.739387750626]\n",
      "Epoch [11/30], Discriminator 0.5893, Best Generator[0] 4.9651\n",
      "[4222.203006505966, 4051.3419301509857, 4170.006492137909]\n",
      "Epoch [12/30], Discriminator 1.1621, Best Generator[0] 17.2142\n",
      "[5492.849962234497, 5427.073870539665, 4839.064697265625]\n",
      "Epoch [13/30], Discriminator 0.0913, Best Generator[0] 7.3060\n",
      "[4611.771765619516, 4733.742370957509, 4664.4086432904005]\n",
      "Epoch [14/30], Discriminator 0.2691, Best Generator[1] 4.8845\n",
      "[4432.977836489677, 4417.65284371376, 4192.394453763962]\n",
      "Epoch [15/30], Discriminator 0.0894, Best Generator[0] 6.8039\n",
      "[4723.807048559189, 4563.72494328022, 4445.247865438461]\n",
      "Epoch [16/30], Discriminator 0.7500, Best Generator[0] 9.5919\n",
      "[4690.602217078209, 4660.360957622528, 4699.070020198822]\n",
      "Epoch [17/30], Discriminator 0.2844, Best Generator[2] 5.7593\n",
      "[4864.760844707489, 4663.568114280701, 4453.3721425533295]\n",
      "Epoch [18/30], Discriminator 0.2213, Best Generator[0] 7.3573\n",
      "[4548.877392053604, 4474.855151176453, 4707.736629724503]\n",
      "Epoch [19/30], Discriminator 0.0766, Best Generator[2] 6.8130\n",
      "[4933.887148976326, 4818.721020698547, 4738.2347729206085]\n",
      "Epoch [20/30], Discriminator 0.1333, Best Generator[0] 6.4717\n"
     ]
    }
   ],
   "source": [
    "# Load model instead of deep copy\n",
    "train(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[856.9920598268509, 803.1024285107851, 851.8648067414761]\n",
      "Epoch [1/40], Discriminator 0.4304, Best Generator[1] 4.6023\n",
      "[1054.6738081872463, 1334.7822082042694, 1052.1758828014135]\n",
      "Epoch [2/40], Discriminator 0.9626, Best Generator[2] 0.2424\n",
      "[1736.5196833498776, 1736.0395818315446, 1815.742018699646]\n",
      "Epoch [3/40], Discriminator 0.3986, Best Generator[1] 0.2782\n",
      "[1382.5161429196596, 1580.1552543640137, 1371.2793620675802]\n",
      "Epoch [4/40], Discriminator 0.9156, Best Generator[2] 0.3780\n",
      "[1281.1298932731152, 1282.6176551878452, 1666.1951867341995]\n",
      "Epoch [5/40], Discriminator 0.3574, Best Generator[0] 0.1930\n",
      "[1786.1965844631195, 870.0297492556274, 869.3980573303998]\n",
      "Epoch [6/40], Discriminator 0.3847, Best Generator[2] 0.1438\n",
      "[843.1533146132715, 844.1812381210038, 1685.6768636703491]\n",
      "Epoch [7/40], Discriminator 0.7598, Best Generator[0] 0.0019\n",
      "[1026.7727580076316, 272.8395392104285, 274.05790696816985]\n",
      "Epoch [8/40], Discriminator 0.4641, Best Generator[1] 0.1701\n",
      "[1738.527152934541, 1527.2031897902489, 1724.3378050304746]\n",
      "Epoch [9/40], Discriminator 0.5665, Best Generator[1] 1.9537\n",
      "[920.9236476421356, 1258.8692388534546, 913.5881178975105]\n",
      "Epoch [10/40], Discriminator 0.4006, Best Generator[2] 0.8905\n",
      "[1136.4622035697103, 1145.7641117274761, 1592.346683382988]\n",
      "Epoch [11/40], Discriminator 0.5943, Best Generator[0] 0.3178\n",
      "[1756.3262034654617, 1271.7948278188705, 1272.1038897708058]\n",
      "Epoch [12/40], Discriminator 0.3991, Best Generator[1] 0.1240\n",
      "[1078.137532569468, 1880.5661512762308, 1079.5790440663695]\n",
      "Epoch [13/40], Discriminator 0.4811, Best Generator[0] 0.1000\n",
      "[1835.7224929779768, 1747.5766119211912, 1749.5129355490208]\n",
      "Epoch [14/40], Discriminator 0.2871, Best Generator[1] 0.1957\n",
      "[1300.0840281248093, 1984.7457265257835, 1287.8278271108866]\n",
      "Epoch [15/40], Discriminator 0.2966, Best Generator[2] 0.2050\n",
      "[1900.1598555305973, 1905.7714022286236, 2122.0290681123734]\n",
      "Epoch [16/40], Discriminator 0.3097, Best Generator[0] 0.0930\n",
      "[2104.9517334401608, 953.032540893415, 950.6426187164616]\n",
      "Epoch [17/40], Discriminator 0.3028, Best Generator[2] 0.0428\n",
      "[971.2384094954468, 974.5227369037457, 2190.4541893601418]\n",
      "Epoch [18/40], Discriminator 0.3790, Best Generator[0] 0.0040\n",
      "[2240.0602054297924, 1881.6838817619719, 1880.0753574676346]\n",
      "Epoch [19/40], Discriminator 0.3677, Best Generator[2] 0.0031\n",
      "[1195.872444877401, 1196.859419381246, 2290.4458743650466]\n",
      "Epoch [20/40], Discriminator 0.1879, Best Generator[0] 0.1309\n",
      "[2459.196710050106, 777.2364097826649, 780.2356910030358]\n",
      "Epoch [21/40], Discriminator 0.3078, Best Generator[1] 0.1586\n",
      "[1559.4249731362215, 2646.826313316822, 1562.2235896728234]\n",
      "Epoch [22/40], Discriminator 0.1649, Best Generator[0] 0.0006\n",
      "[2341.016333884094, 1349.4908470744267, 1346.286815050291]\n",
      "Epoch [23/40], Discriminator 0.2307, Best Generator[2] 0.1007\n",
      "[1883.9395901672542, 1885.8269568514079, 2475.2950426340103]\n",
      "Epoch [24/40], Discriminator 0.1376, Best Generator[0] 0.0681\n",
      "[2731.6187206208706, 1147.1349485386163, 1140.6236384361982]\n",
      "Epoch [25/40], Discriminator 0.2461, Best Generator[2] 0.1127\n",
      "[1757.7430075146258, 1762.2815098781139, 3035.5109964609146]\n",
      "Epoch [26/40], Discriminator 0.2748, Best Generator[0] 0.0215\n",
      "[2676.661723136902, 2221.658841662109, 2218.0906108915806]\n",
      "Epoch [27/40], Discriminator 0.1649, Best Generator[2] 0.0467\n",
      "[1278.6739176474512, 1281.5947484187782, 2688.160478055477]\n",
      "Epoch [28/40], Discriminator 0.2426, Best Generator[0] 0.1304\n",
      "[2792.371824860573, 2123.935339411022, 2123.752914842218]\n",
      "Epoch [29/40], Discriminator 0.3262, Best Generator[2] 0.0068\n",
      "[1411.93895589374, 1411.628830594942, 2611.823527369648]\n",
      "Epoch [30/40], Discriminator 0.3122, Best Generator[1] 0.1149\n",
      "[1831.1382739953697, 2863.792358517647, 1827.003133090213]\n",
      "Epoch [31/40], Discriminator 0.4056, Best Generator[2] 0.0276\n",
      "[1500.5683856757823, 1504.1713867945946, 2862.48747728765]\n",
      "Epoch [32/40], Discriminator 0.2974, Best Generator[0] 0.0014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3058ca475f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# With optimizer on the fly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-8a95a2cb66ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(BestPerformingGenerator, num_epochs, d_iter)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlossList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNumberOfGenerators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/torchvision-0.2.0-py2.7.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2443\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2445\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2389\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# may change to (mode, 0, 1) post-1.1.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2391\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m             im = im._new(\n\u001b[1;32m   2393\u001b[0m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2292\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageColor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bhushan/anaconda3/envs/py27/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0m_close_exclusive_fp_after_loading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m         \u001b[0;31m# FIXME: take \"new\" parameters / other image?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# FIXME: turn mode and size into delegating properties?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With optimizer on the fly\n",
    "train(0, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[872.1570700109005, 662.0861651301384, 609.2562948316336]\n",
      "Epoch [1/30], Discriminator 0.6847, Best Generator[0] 1.7367\n",
      "[814.5087671279907, 751.060758471489, 748.6468422412872]\n",
      "Epoch [2/30], Discriminator 1.0386, Best Generator[0] 1.1394\n",
      "[1001.8705675601959, 402.21531569585204, 399.4936091527343]\n",
      "Epoch [3/30], Discriminator 0.8658, Best Generator[0] 2.0115\n",
      "[1254.4419853687286, 850.1604254692793, 845.96289460361]\n",
      "Epoch [4/30], Discriminator 0.4126, Best Generator[0] 2.5391\n",
      "[1661.9885839223862, 1183.961763634812, 1223.4222673280165]\n",
      "Epoch [5/30], Discriminator 0.4755, Best Generator[0] 3.0569\n",
      "[1639.9664514064789, 1637.8508403003216, 1646.1216702461243]\n",
      "Epoch [6/30], Discriminator 0.5279, Best Generator[2] 0.4772\n",
      "[1645.3805298805237, 1632.3828236460686, 1544.1907473802567]\n",
      "Epoch [7/30], Discriminator 0.4334, Best Generator[0] 0.8038\n",
      "[1624.6379365324974, 1345.9067988991737, 1343.315298974514]\n",
      "Epoch [8/30], Discriminator 0.5002, Best Generator[0] 2.7455\n",
      "[2061.575703859329, 1457.665353178978, 1395.583261191845]\n",
      "Epoch [9/30], Discriminator 0.5804, Best Generator[0] 2.5811\n",
      "[1755.7334052324295, 1126.5460166931152, 1138.7776359915733]\n",
      "Epoch [10/30], Discriminator 0.4904, Best Generator[0] 3.1247\n",
      "[1841.2697052955627, 1111.8885192871094, 1095.6448769569397]\n",
      "Epoch [11/30], Discriminator 0.3857, Best Generator[0] 4.2212\n",
      "[2037.2302458286285, 1630.1399109959602, 1612.7301331460476]\n",
      "Epoch [12/30], Discriminator 0.3690, Best Generator[0] 3.2038\n",
      "[2134.0492448806763, 1313.1974902749062, 1295.0818182229996]\n",
      "Epoch [13/30], Discriminator 0.2966, Best Generator[0] 3.1558\n",
      "[2251.631275177002, 1256.6901438832283, 1264.8015676736832]\n",
      "Epoch [14/30], Discriminator 0.2301, Best Generator[0] 4.0189\n",
      "[2211.767878293991, 1462.0209045410156, 1473.4623212218285]\n",
      "Epoch [15/30], Discriminator 0.3156, Best Generator[0] 3.8568\n",
      "[2252.7424585819244, 1604.3273143172264, 1639.3052503466606]\n",
      "Epoch [16/30], Discriminator 0.3686, Best Generator[0] 4.0857\n",
      "[2232.465948343277, 1760.5494663119316, 1768.2756642103195]\n",
      "Epoch [17/30], Discriminator 0.4316, Best Generator[0] 3.9720\n",
      "[2446.165271759033, 1822.210275053978, 1884.2799251675606]\n",
      "Epoch [18/30], Discriminator 0.1943, Best Generator[0] 3.9451\n",
      "[2395.9150722026825, 1351.4331141710281, 1326.8580349683762]\n",
      "Epoch [19/30], Discriminator 0.4404, Best Generator[0] 4.1073\n",
      "[2348.1448237895966, 1273.567288517952, 1311.2974657416344]\n",
      "Epoch [20/30], Discriminator 0.2960, Best Generator[0] 3.5321\n",
      "[2409.535304546356, 1782.260391652584, 1728.586496591568]\n",
      "Epoch [21/30], Discriminator 0.2539, Best Generator[0] 4.9073\n",
      "[2449.4630858898163, 1369.4856013059616, 1399.2090330123901]\n",
      "Epoch [22/30], Discriminator 0.1901, Best Generator[0] 4.7282\n",
      "[2473.216210126877, 1619.7988632321358, 1623.273171544075]\n",
      "Epoch [23/30], Discriminator 0.2810, Best Generator[0] 4.3101\n",
      "[2560.063740491867, 1665.7799058556557, 1723.9129244089127]\n",
      "Epoch [24/30], Discriminator 0.1581, Best Generator[0] 4.2523\n",
      "[2513.8959436416626, 1548.1140825748444, 1536.6563726067543]\n",
      "Epoch [25/30], Discriminator 0.4184, Best Generator[0] 3.7386\n",
      "[2554.2282292842865, 1642.1847410798073, 1683.7064735293388]\n",
      "Epoch [26/30], Discriminator 0.3158, Best Generator[0] 4.5012\n",
      "[2538.237756729126, 1732.5801632404327, 1736.3425637483597]\n",
      "Epoch [27/30], Discriminator 0.1435, Best Generator[0] 4.5066\n",
      "[2551.242653131485, 1674.7433705329895, 1642.2202155590057]\n",
      "Epoch [28/30], Discriminator 0.3953, Best Generator[0] 4.4175\n",
      "[2608.9233951568604, 1636.0791056752205, 1719.6294510364532]\n",
      "Epoch [29/30], Discriminator 0.2822, Best Generator[0] 4.3083\n",
      "[2585.7635400295258, 1774.507214486599, 1787.6149329543114]\n",
      "Epoch [30/30], Discriminator 0.3816, Best Generator[0] 4.3661\n"
     ]
    }
   ],
   "source": [
    "# With optimizer on the fly and max loss\n",
    "train(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2778.058117568493, 2917.6230896115303, 2697.726538479328]\n",
      "Epoch [1/30], Discriminator 0.0416, Best Generator[1] 5.6720\n",
      "[3695.9072647094727, 3356.031274318695, 3374.9563450813293]\n",
      "Epoch [2/30], Discriminator 0.1799, Best Generator[0] 6.3682\n",
      "[3256.957720041275, 3510.96928858757, 3557.807953119278]\n",
      "Epoch [3/30], Discriminator 0.0258, Best Generator[2] 6.8425\n",
      "[4142.80516242981, 4470.754581928253, 3614.0237057209015]\n",
      "Epoch [4/30], Discriminator 0.0879, Best Generator[1] 6.4516\n",
      "[4529.959128856659, 3948.716283798218, 4235.133533000946]\n",
      "Epoch [5/30], Discriminator 0.0387, Best Generator[0] 8.0247\n",
      "[4343.090248584747, 4722.502594470978, 4257.555803775787]\n",
      "Epoch [6/30], Discriminator 0.2769, Best Generator[1] 5.8655\n",
      "[4618.889126300812, 4463.452397823334, 4359.163161277771]\n",
      "Epoch [7/30], Discriminator 0.2528, Best Generator[0] 6.2346\n",
      "[4684.915420532227, 4336.271741390228, 4291.608334541321]\n",
      "Epoch [8/30], Discriminator 0.1100, Best Generator[0] 7.7472\n",
      "[4734.094176769257, 4796.5427713394165, 4795.7561893463135]\n",
      "Epoch [9/30], Discriminator 0.1253, Best Generator[1] 6.9403\n",
      "[5049.959019184113, 5027.6559336185455, 5120.212819099426]\n",
      "Epoch [10/30], Discriminator 0.0754, Best Generator[2] 6.8321\n",
      "[5174.600156784058, 5256.379452705383, 5157.375615596771]\n",
      "Epoch [11/30], Discriminator 0.1244, Best Generator[1] 7.6583\n",
      "[5001.470520496368, 5049.887799739838, 4872.8134207725525]\n",
      "Epoch [12/30], Discriminator 0.0142, Best Generator[1] 7.1265\n",
      "[5400.529816150665, 5309.533566951752, 5343.8241357803345]\n",
      "Epoch [13/30], Discriminator 0.0229, Best Generator[0] 8.3038\n",
      "[5223.159967422485, 5364.416179656982, 5361.684566497803]\n",
      "Epoch [14/30], Discriminator 0.0364, Best Generator[1] 8.7717\n",
      "[5417.0434284210205, 5463.120951652527, 4973.908950805664]\n",
      "Epoch [15/30], Discriminator 0.0309, Best Generator[1] 8.7464\n",
      "[5653.333395957947, 5171.138589859009, 5531.204066753387]\n",
      "Epoch [16/30], Discriminator 0.0559, Best Generator[0] 8.1740\n",
      "[5531.024948596954, 5656.0243310928345, 5634.804122924805]\n",
      "Epoch [17/30], Discriminator 0.0575, Best Generator[1] 7.7896\n",
      "[5680.24044418335, 5400.6087074279785, 5439.811349391937]\n",
      "Epoch [18/30], Discriminator 0.0341, Best Generator[0] 7.5349\n",
      "[5360.651843070984, 5336.985419750214, 5450.201311588287]\n",
      "Epoch [19/30], Discriminator 0.0186, Best Generator[2] 7.9655\n",
      "[5578.952021598816, 5580.399248123169, 5202.668723583221]\n",
      "Epoch [20/30], Discriminator 0.0143, Best Generator[1] 8.4579\n",
      "[5301.0527811050415, 5475.551202774048, 5285.052348613739]\n",
      "Epoch [21/30], Discriminator 0.0782, Best Generator[1] 8.3499\n",
      "[6101.800065040588, 5427.678186416626, 5899.076765537262]\n",
      "Epoch [22/30], Discriminator 0.0909, Best Generator[0] 10.0620\n",
      "[5653.270559310913, 5647.117569923401, 5603.526169300079]\n",
      "Epoch [23/30], Discriminator 0.0300, Best Generator[0] 10.4779\n",
      "[5648.005443096161, 5485.590974807739, 5443.540237903595]\n",
      "Epoch [24/30], Discriminator 0.0662, Best Generator[0] 10.5627\n",
      "[5732.1653509140015, 5481.707170963287, 5396.85217666626]\n",
      "Epoch [25/30], Discriminator 0.0424, Best Generator[0] 11.1002\n",
      "[5923.800654411316, 6075.984002113342, 6023.765144824982]\n",
      "Epoch [26/30], Discriminator 0.0601, Best Generator[1] 8.8528\n",
      "[6104.5225286483765, 5588.120784282684, 6018.230801582336]\n",
      "Epoch [27/30], Discriminator 0.0528, Best Generator[0] 6.8982\n",
      "[5405.457807064056, 5598.817863941193, 5503.100583076477]\n",
      "Epoch [28/30], Discriminator 0.0105, Best Generator[1] 9.6997\n",
      "[5740.616246700287, 5446.834491729736, 5789.677504062653]\n",
      "Epoch [29/30], Discriminator 0.0117, Best Generator[2] 13.0937\n",
      "[5986.13765668869, 5840.38082075119, 5535.985820770264]\n",
      "Epoch [30/30], Discriminator 0.0173, Best Generator[0] 11.2642\n"
     ]
    }
   ],
   "source": [
    "train(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(G.state_dict(), './Generator.pkl')\n",
    "#torch.save(D.state_dict(), './Discriminator.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator.load_state_dict(torch.load('Generator200.pkl'))\n",
    "#Discriminator.load_state_dict(torch.load('Discriminator200.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import imageio\n",
    "#imageio.mimsave(path + 'MNIST_MaxNew_{:d}'.format(30) + '.gif', outputImages, fps=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
