{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = datasets.MNIST(root='./data',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "test_dataset = datasets.MNIST(root='./data',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "# Data loader\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    IS_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var(x):\n",
    "    if IS_CUDA:\n",
    "        x = x.cuda()\n",
    "    return Variable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: standard model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(h_dim, z_dim*2))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + epsilon * torch.exp(log_var/2)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim = 1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "vae = VAE()\n",
    "if IS_CUDA:\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: New Layer for Mean and Variance\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.MuExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.SigmaExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + epsilon * torch.exp(log_var/2)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.MuExtractor(h)\n",
    "        log_var = self.SigmaExtractor(h)\n",
    "        # mu, log_var = torch.chunk(h, 2, dim = 1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "vae = VAE()\n",
    "if IS_CUDA:\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Drop Mu size by Half and skip earlier middle layer\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim/2, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, image_size),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.MuExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim/2),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.SigmaExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, z_dim/2),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + epsilon * torch.exp(log_var/2)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.MuExtractor(h)\n",
    "        log_var = self.SigmaExtractor(h)\n",
    "        # mu, log_var = torch.chunk(h, 2, dim = 1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "vae = VAE()\n",
    "if IS_CUDA:\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Get single value for Mean and Variance\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(z_dim, image_size),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.MuExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.SigmaExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + epsilon * torch.exp(log_var/2)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.MuExtractor(h)\n",
    "        log_var = self.SigmaExtractor(h)\n",
    "        # mu, log_var = torch.chunk(h, 2, dim = 1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "vae = VAE()\n",
    "if IS_CUDA:\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5: Drop Mean and Variance to 10\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim = 400, z_dim = 20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, h_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(h_dim, z_dim))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, z_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(z_dim, image_size),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.MuExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, 10))\n",
    "            #nn.Sigmoid())\n",
    "        \n",
    "        self.SigmaExtractor = nn.Sequential(\n",
    "            nn.Linear(z_dim, 10))\n",
    "            #nn.Sigmoid())\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        epsilon = var(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + epsilon * torch.exp(log_var/2)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.MuExtractor(h)\n",
    "        log_var = self.SigmaExtractor(h)\n",
    "        # mu, log_var = torch.chunk(h, 2, dim = 1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, log_var\n",
    "    \n",
    "    def sample(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "vae = VAE()\n",
    "if IS_CUDA:\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loader)\n",
    "fixed_x,_ = next(data_iter)\n",
    "torchvision.utils.save_image(fixed_x.cpu(), './data/real_images.png')\n",
    "fixed_x = var(fixed_x.view(fixed_x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "# os.mkdir('./data/genImg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(outputImages, modelName):\n",
    "    path = './data/genImg/'+modelName\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in data_loader:\n",
    "            img, _ = data\n",
    "            img = var(img.view(img.size(0), -1))\n",
    "            out, mu, log_var = vae(img)\n",
    "            rc_loss = F.binary_cross_entropy(out, img, size_average=False)\n",
    "            KL_div = torch.sum(0.5 * (mu ** 2 + torch.exp(log_var) - log_var - 1))\n",
    "\n",
    "            total_loss = rc_loss + KL_div\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print 'Epoch [{}/{}], Loss {:.4f}, Entropy: {:.4f}, KL: {: .4f} '.format(epoch+1, num_epochs, total_loss.data[0], rc_loss.data[0], KL_div.data[0])\n",
    "        pic, _, _ = vae(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, 28, 28) \n",
    "        outputImages.append(pic)\n",
    "        torchvision.utils.save_image(pic.data.cpu(), path+'/image_{}.png'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test():\n",
    "    i = 0\n",
    "    outputImages = []\n",
    "    labels = []\n",
    "    for data in test_data_loader:\n",
    "        img, label = data\n",
    "        img = var(img.view(img.size(0), -1))\n",
    "        out, mu, log_var = vae(img)\n",
    "        rc_loss = F.binary_cross_entropy(out, img, size_average=False)\n",
    "        KL_div = torch.sum(0.5 * (mu ** 2 + torch.exp(log_var) - log_var - 1))\n",
    "\n",
    "        total_loss = rc_loss + KL_div\n",
    "        i += 1\n",
    "        print i, ' Loss {:.4f}, Entropy: {:.4f}, KL: {: .4f} '.format(total_loss.data[0], rc_loss.data[0], KL_div.data[0])\n",
    "        pic, _, _ = vae(fixed_x)\n",
    "        pic = pic.view(pic.size(0), 1, 28, 28) \n",
    "        outputImages.append((out.data).cpu().numpy())\n",
    "        labels.append(label)\n",
    "    return outputImages, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss 9966.2021, Entropy: 7515.0522, KL:  2451.1497 \n",
      "Epoch [2/30], Loss 10149.4375, Entropy: 7607.4604, KL:  2541.9775 \n",
      "Epoch [3/30], Loss 10157.8486, Entropy: 7572.2407, KL:  2585.6082 \n",
      "Epoch [4/30], Loss 10190.3379, Entropy: 7592.8374, KL:  2597.5000 \n",
      "Epoch [5/30], Loss 9832.0234, Entropy: 7341.6924, KL:  2490.3313 \n",
      "Epoch [6/30], Loss 10243.3896, Entropy: 7697.5723, KL:  2545.8171 \n",
      "Epoch [7/30], Loss 10261.2461, Entropy: 7684.6406, KL:  2576.6050 \n",
      "Epoch [8/30], Loss 10315.4219, Entropy: 7757.6201, KL:  2557.8018 \n",
      "Epoch [9/30], Loss 10235.6445, Entropy: 7785.2031, KL:  2450.4412 \n",
      "Epoch [10/30], Loss 10040.8350, Entropy: 7535.2700, KL:  2505.5652 \n",
      "Epoch [11/30], Loss 10562.3320, Entropy: 8065.3008, KL:  2497.0308 \n",
      "Epoch [12/30], Loss 10246.7002, Entropy: 7674.1450, KL:  2572.5554 \n",
      "Epoch [13/30], Loss 10396.9424, Entropy: 7842.6689, KL:  2554.2737 \n",
      "Epoch [14/30], Loss 10202.7773, Entropy: 7727.4321, KL:  2475.3450 \n",
      "Epoch [15/30], Loss 10596.3613, Entropy: 8013.4712, KL:  2582.8899 \n",
      "Epoch [16/30], Loss 9977.3047, Entropy: 7437.7368, KL:  2539.5679 \n",
      "Epoch [17/30], Loss 10012.8213, Entropy: 7516.4336, KL:  2496.3879 \n",
      "Epoch [18/30], Loss 9947.4883, Entropy: 7460.0229, KL:  2487.4651 \n",
      "Epoch [19/30], Loss 10467.1211, Entropy: 7892.4414, KL:  2574.6792 \n",
      "Epoch [20/30], Loss 9896.5459, Entropy: 7355.2939, KL:  2541.2522 \n",
      "Epoch [21/30], Loss 10322.7246, Entropy: 7752.8184, KL:  2569.9065 \n",
      "Epoch [22/30], Loss 10088.0938, Entropy: 7521.1089, KL:  2566.9849 \n",
      "Epoch [23/30], Loss 9974.1270, Entropy: 7441.4932, KL:  2532.6338 \n",
      "Epoch [24/30], Loss 10338.7930, Entropy: 7696.3354, KL:  2642.4580 \n",
      "Epoch [25/30], Loss 9922.8008, Entropy: 7380.9780, KL:  2541.8232 \n",
      "Epoch [26/30], Loss 10010.5312, Entropy: 7545.1870, KL:  2465.3438 \n",
      "Epoch [27/30], Loss 10281.0586, Entropy: 7720.8462, KL:  2560.2126 \n",
      "Epoch [28/30], Loss 9872.3496, Entropy: 7388.2998, KL:  2484.0493 \n",
      "Epoch [29/30], Loss 10240.0293, Entropy: 7705.1704, KL:  2534.8584 \n",
      "Epoch [30/30], Loss 10494.2969, Entropy: 7860.9331, KL:  2633.3640 \n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "outputImages = []\n",
    "train(outputImages, 'ExtractFromLayer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  Loss 10230.0762, Entropy: 7683.6147, KL:  2546.4614 \n",
      "2  Loss 10663.1680, Entropy: 8070.4014, KL:  2592.7671 \n",
      "3  Loss 10300.2285, Entropy: 7728.0835, KL:  2572.1445 \n",
      "4  Loss 9814.8135, Entropy: 7330.0122, KL:  2484.8013 \n",
      "5  Loss 10073.9727, Entropy: 7487.5088, KL:  2586.4639 \n",
      "6  Loss 10463.7324, Entropy: 7845.9434, KL:  2617.7886 \n",
      "7  Loss 10432.4199, Entropy: 7829.6670, KL:  2602.7529 \n",
      "8  Loss 10131.3926, Entropy: 7623.0684, KL:  2508.3245 \n",
      "9  Loss 10031.3066, Entropy: 7550.1870, KL:  2481.1201 \n",
      "10  Loss 9751.9102, Entropy: 7330.5024, KL:  2421.4080 \n",
      "11  Loss 9910.4756, Entropy: 7395.8696, KL:  2514.6057 \n",
      "12  Loss 10346.1104, Entropy: 7763.2529, KL:  2582.8572 \n",
      "13  Loss 10368.8975, Entropy: 7782.8101, KL:  2586.0872 \n",
      "14  Loss 10154.0918, Entropy: 7635.8120, KL:  2518.2798 \n",
      "15  Loss 9963.4238, Entropy: 7446.4580, KL:  2516.9656 \n",
      "16  Loss 9666.5293, Entropy: 7160.4912, KL:  2506.0381 \n",
      "17  Loss 10263.4014, Entropy: 7695.4229, KL:  2567.9783 \n",
      "18  Loss 9630.1650, Entropy: 7184.4604, KL:  2445.7046 \n",
      "19  Loss 10152.8330, Entropy: 7608.1343, KL:  2544.6987 \n",
      "20  Loss 10199.5508, Entropy: 7669.6221, KL:  2529.9282 \n",
      "21  Loss 10218.0234, Entropy: 7666.4595, KL:  2551.5640 \n",
      "22  Loss 10143.9424, Entropy: 7580.7437, KL:  2563.1990 \n",
      "23  Loss 9875.2871, Entropy: 7351.9663, KL:  2523.3203 \n",
      "24  Loss 10001.2949, Entropy: 7492.8560, KL:  2508.4385 \n",
      "25  Loss 9695.5742, Entropy: 7278.2100, KL:  2417.3647 \n",
      "26  Loss 10043.9775, Entropy: 7521.9463, KL:  2522.0312 \n",
      "27  Loss 10319.1748, Entropy: 7824.2075, KL:  2494.9670 \n",
      "28  Loss 10243.3857, Entropy: 7736.2383, KL:  2507.1475 \n",
      "29  Loss 10305.8789, Entropy: 7705.6621, KL:  2600.2163 \n",
      "30  Loss 10508.5371, Entropy: 7964.7490, KL:  2543.7876 \n",
      "31  Loss 10278.6641, Entropy: 7720.6494, KL:  2558.0149 \n",
      "32  Loss 10213.3242, Entropy: 7647.3125, KL:  2566.0120 \n",
      "33  Loss 10107.9170, Entropy: 7542.3872, KL:  2565.5300 \n",
      "34  Loss 10415.0693, Entropy: 7853.0806, KL:  2561.9888 \n",
      "35  Loss 10140.4297, Entropy: 7615.8921, KL:  2524.5376 \n",
      "36  Loss 9797.3682, Entropy: 7293.3223, KL:  2504.0461 \n",
      "37  Loss 10432.5176, Entropy: 7832.1162, KL:  2600.4014 \n",
      "38  Loss 9952.3105, Entropy: 7448.3652, KL:  2503.9451 \n",
      "39  Loss 10487.7471, Entropy: 7909.2178, KL:  2578.5293 \n",
      "40  Loss 9917.7402, Entropy: 7382.5049, KL:  2535.2349 \n",
      "41  Loss 9769.7041, Entropy: 7275.0586, KL:  2494.6458 \n",
      "42  Loss 10164.0195, Entropy: 7652.0737, KL:  2511.9458 \n",
      "43  Loss 10751.2100, Entropy: 8142.1953, KL:  2609.0146 \n",
      "44  Loss 10476.5547, Entropy: 7876.0903, KL:  2600.4639 \n",
      "45  Loss 10126.0625, Entropy: 7580.4155, KL:  2545.6465 \n",
      "46  Loss 10372.9355, Entropy: 7788.2397, KL:  2584.6956 \n",
      "47  Loss 10005.9404, Entropy: 7479.1406, KL:  2526.7998 \n",
      "48  Loss 10225.9297, Entropy: 7677.3896, KL:  2548.5405 \n",
      "49  Loss 10175.8750, Entropy: 7600.0327, KL:  2575.8425 \n",
      "50  Loss 10103.3535, Entropy: 7539.9556, KL:  2563.3977 \n",
      "51  Loss 10088.7461, Entropy: 7592.3115, KL:  2496.4343 \n",
      "52  Loss 9782.1650, Entropy: 7268.5947, KL:  2513.5706 \n",
      "53  Loss 9983.3857, Entropy: 7425.2241, KL:  2558.1619 \n",
      "54  Loss 10230.1172, Entropy: 7702.2354, KL:  2527.8816 \n",
      "55  Loss 10347.8770, Entropy: 7732.9077, KL:  2614.9688 \n",
      "56  Loss 10217.2314, Entropy: 7635.5078, KL:  2581.7234 \n",
      "57  Loss 10579.8896, Entropy: 7986.8662, KL:  2593.0232 \n",
      "58  Loss 9964.0371, Entropy: 7495.5161, KL:  2468.5212 \n",
      "59  Loss 10648.9424, Entropy: 8029.2402, KL:  2619.7024 \n",
      "60  Loss 10533.8535, Entropy: 7925.3540, KL:  2608.5000 \n",
      "61  Loss 9547.5254, Entropy: 7121.1343, KL:  2426.3916 \n",
      "62  Loss 10352.1895, Entropy: 7788.2578, KL:  2563.9316 \n",
      "63  Loss 9893.9609, Entropy: 7422.4131, KL:  2471.5481 \n",
      "64  Loss 10064.6143, Entropy: 7555.6968, KL:  2508.9172 \n",
      "65  Loss 10021.1797, Entropy: 7497.9546, KL:  2523.2246 \n",
      "66  Loss 10501.7422, Entropy: 7894.4189, KL:  2607.3232 \n",
      "67  Loss 10353.5195, Entropy: 7759.6592, KL:  2593.8601 \n",
      "68  Loss 10483.4453, Entropy: 7904.1802, KL:  2579.2649 \n",
      "69  Loss 10377.0225, Entropy: 7807.4805, KL:  2569.5417 \n",
      "70  Loss 10051.5449, Entropy: 7526.0400, KL:  2525.5049 \n",
      "71  Loss 10195.9678, Entropy: 7657.5039, KL:  2538.4636 \n",
      "72  Loss 10317.8848, Entropy: 7747.8799, KL:  2570.0049 \n",
      "73  Loss 10126.1650, Entropy: 7569.2949, KL:  2556.8704 \n",
      "74  Loss 9537.3535, Entropy: 7118.1548, KL:  2419.1985 \n",
      "75  Loss 10479.9707, Entropy: 7866.8511, KL:  2613.1199 \n",
      "76  Loss 10469.3633, Entropy: 7893.0063, KL:  2576.3567 \n",
      "77  Loss 10353.0977, Entropy: 7767.0254, KL:  2586.0718 \n",
      "78  Loss 10234.5742, Entropy: 7684.9595, KL:  2549.6150 \n",
      "79  Loss 10095.0352, Entropy: 7549.3257, KL:  2545.7095 \n",
      "80  Loss 9898.6973, Entropy: 7410.8164, KL:  2487.8804 \n",
      "81  Loss 10350.4414, Entropy: 7749.4614, KL:  2600.9800 \n",
      "82  Loss 10129.5049, Entropy: 7594.3159, KL:  2535.1890 \n",
      "83  Loss 10138.8516, Entropy: 7603.1484, KL:  2535.7036 \n",
      "84  Loss 10331.7598, Entropy: 7706.4683, KL:  2625.2910 \n",
      "85  Loss 9959.8926, Entropy: 7448.8198, KL:  2511.0728 \n",
      "86  Loss 9880.3311, Entropy: 7379.3574, KL:  2500.9739 \n",
      "87  Loss 10634.5264, Entropy: 8030.3760, KL:  2604.1501 \n",
      "88  Loss 10223.5918, Entropy: 7667.9038, KL:  2555.6885 \n",
      "89  Loss 10283.8643, Entropy: 7712.5449, KL:  2571.3196 \n",
      "90  Loss 10208.2363, Entropy: 7608.4004, KL:  2599.8354 \n",
      "91  Loss 10056.9541, Entropy: 7551.5381, KL:  2505.4160 \n",
      "92  Loss 10012.5967, Entropy: 7485.8979, KL:  2526.6990 \n",
      "93  Loss 10477.2959, Entropy: 7892.9248, KL:  2584.3711 \n",
      "94  Loss 9607.6221, Entropy: 7151.5889, KL:  2456.0330 \n",
      "95  Loss 10267.1953, Entropy: 7759.4150, KL:  2507.7808 \n",
      "96  Loss 10416.0869, Entropy: 7907.2388, KL:  2508.8481 \n",
      "97  Loss 10116.0381, Entropy: 7579.7988, KL:  2536.2390 \n",
      "98  Loss 10675.3066, Entropy: 8068.1875, KL:  2607.1194 \n",
      "99  Loss 10173.3174, Entropy: 7648.3032, KL:  2525.0144 \n",
      "100  Loss 10515.2500, Entropy: 7893.1138, KL:  2622.1357 \n"
     ]
    }
   ],
   "source": [
    "predicted, labels = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npNum = np.array(predicted)\\nlNum = np.array(labels)\\nprint pNum.shape, lNum.shape\\n\\nx_test_encoded = pNum\\nplt.figure(figsize=(6, 6))\\nplt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=lNum)\\nplt.colorbar()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "'''\n",
    "pNum = np.array(predicted)\n",
    "lNum = np.array(labels)\n",
    "print pNum.shape, lNum.shape\n",
    "\n",
    "x_test_encoded = pNum\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=lNum)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 18742.7168, Entropy: 18325.1270, KL:  417.5903 \n",
      "Epoch [2/10], Loss 18859.0566, Entropy: 18439.0176, KL:  420.0390 \n",
      "Epoch [3/10], Loss 17914.0098, Entropy: 17477.4941, KL:  436.5148 \n",
      "Epoch [4/10], Loss 19026.9043, Entropy: 18609.7520, KL:  417.1521 \n",
      "Epoch [5/10], Loss 18778.0293, Entropy: 18364.2324, KL:  413.7972 \n",
      "Epoch [6/10], Loss 18179.8105, Entropy: 17758.8984, KL:  420.9129 \n",
      "Epoch [7/10], Loss 18144.7070, Entropy: 17730.5508, KL:  414.1560 \n",
      "Epoch [8/10], Loss 18379.2988, Entropy: 17948.3750, KL:  430.9247 \n",
      "Epoch [9/10], Loss 19538.9688, Entropy: 19130.7754, KL:  408.1940 \n",
      "Epoch [10/10], Loss 19493.7598, Entropy: 19092.1016, KL:  401.6581 \n"
     ]
    }
   ],
   "source": [
    "# Model 2\n",
    "outputImages = []\n",
    "train(outputImages, 'SameSizeNNDiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 19722.6504, Entropy: 19512.7930, KL:  209.8571 \n",
      "Epoch [2/10], Loss 20034.1035, Entropy: 19819.8984, KL:  214.2052 \n",
      "Epoch [3/10], Loss 19196.9199, Entropy: 18977.1758, KL:  219.7438 \n",
      "Epoch [4/10], Loss 18621.2422, Entropy: 18411.8965, KL:  209.3467 \n",
      "Epoch [5/10], Loss 18940.5488, Entropy: 18722.9941, KL:  217.5538 \n",
      "Epoch [6/10], Loss 18762.9883, Entropy: 18537.1777, KL:  225.8106 \n",
      "Epoch [7/10], Loss 20092.7051, Entropy: 19881.1543, KL:  211.5514 \n",
      "Epoch [8/10], Loss 18819.5293, Entropy: 18605.9922, KL:  213.5374 \n",
      "Epoch [9/10], Loss 20054.7402, Entropy: 19841.6758, KL:  213.0651 \n",
      "Epoch [10/10], Loss 18753.1641, Entropy: 18538.4453, KL:  214.7187 \n"
     ]
    }
   ],
   "source": [
    "# Model 3\n",
    "outputImages = []\n",
    "train(outputImages, 'ReduceMuAndSigmaTo10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 20389.1172, Entropy: 20357.9766, KL:  31.1408 \n",
      "Epoch [2/10], Loss 20300.5684, Entropy: 20267.0430, KL:  33.5260 \n",
      "Epoch [3/10], Loss 20817.9375, Entropy: 20790.1562, KL:  27.7818 \n",
      "Epoch [4/10], Loss 21050.6875, Entropy: 21025.8633, KL:  24.8244 \n",
      "Epoch [5/10], Loss 20431.5469, Entropy: 20407.6836, KL:  23.8636 \n",
      "Epoch [6/10], Loss 20090.8750, Entropy: 20061.3945, KL:  29.4803 \n",
      "Epoch [7/10], Loss 20528.7344, Entropy: 20502.5664, KL:  26.1676 \n",
      "Epoch [8/10], Loss 20044.1172, Entropy: 20016.9336, KL:  27.1830 \n",
      "Epoch [9/10], Loss 19901.6699, Entropy: 19871.2285, KL:  30.4405 \n",
      "Epoch [10/10], Loss 20328.8809, Entropy: 20299.1719, KL:  29.7093 \n"
     ]
    }
   ],
   "source": [
    "# Model 4\n",
    "outputImages = []\n",
    "train(outputImages, 'Rto1AndDecoderUnfoldsCorrectly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 19340.7031, Entropy: 19067.6660, KL:  273.0374 \n",
      "Epoch [2/10], Loss 17749.2969, Entropy: 17264.1484, KL:  485.1486 \n",
      "Epoch [3/10], Loss 16665.1934, Entropy: 16013.5635, KL:  651.6296 \n",
      "Epoch [4/10], Loss 16162.4639, Entropy: 15424.8564, KL:  737.6074 \n",
      "Epoch [5/10], Loss 14840.4326, Entropy: 13837.5557, KL:  1002.8772 \n",
      "Epoch [6/10], Loss 14273.4863, Entropy: 13219.9209, KL:  1053.5653 \n",
      "Epoch [7/10], Loss 14007.1719, Entropy: 12828.5762, KL:  1178.5956 \n",
      "Epoch [8/10], Loss 14075.8506, Entropy: 12880.7725, KL:  1195.0779 \n",
      "Epoch [9/10], Loss 13558.8613, Entropy: 12396.0205, KL:  1162.8413 \n",
      "Epoch [10/10], Loss 13036.4082, Entropy: 11771.5039, KL:  1264.9047 \n"
     ]
    }
   ],
   "source": [
    "# Model 5\n",
    "outputImages = []\n",
    "train(outputImages, 'DropTo10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 12766.8984, Entropy: 11467.5771, KL:  1299.3214 \n",
      "Epoch [2/10], Loss 12961.6426, Entropy: 11648.4121, KL:  1313.2308 \n",
      "Epoch [3/10], Loss 13263.9736, Entropy: 11919.2744, KL:  1344.6991 \n",
      "Epoch [4/10], Loss 14032.5889, Entropy: 12692.1475, KL:  1340.4415 \n",
      "Epoch [5/10], Loss 13678.1377, Entropy: 12304.7334, KL:  1373.4045 \n",
      "Epoch [6/10], Loss 12761.2559, Entropy: 11373.9082, KL:  1387.3474 \n",
      "Epoch [7/10], Loss 13742.5078, Entropy: 12310.2256, KL:  1432.2817 \n",
      "Epoch [8/10], Loss 12951.0225, Entropy: 11546.1377, KL:  1404.8848 \n",
      "Epoch [9/10], Loss 13153.7861, Entropy: 11776.6826, KL:  1377.1034 \n",
      "Epoch [10/10], Loss 12787.2734, Entropy: 11379.5977, KL:  1407.6763 \n"
     ]
    }
   ],
   "source": [
    "# Model 5 - It2\n",
    "outputImages = []\n",
    "train(outputImages, 'DropTo10_IT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss 12741.8477, Entropy: 11327.5684, KL:  1414.2793 \n",
      "Epoch [2/10], Loss 12824.7891, Entropy: 11437.7178, KL:  1387.0709 \n",
      "Epoch [3/10], Loss 13878.9473, Entropy: 12441.8457, KL:  1437.1013 \n",
      "Epoch [4/10], Loss 12997.6963, Entropy: 11566.4258, KL:  1431.2701 \n",
      "Epoch [5/10], Loss 12914.2734, Entropy: 11500.6113, KL:  1413.6624 \n",
      "Epoch [6/10], Loss 13183.6943, Entropy: 11820.0420, KL:  1363.6522 \n",
      "Epoch [7/10], Loss 13298.8838, Entropy: 11929.5459, KL:  1369.3383 \n",
      "Epoch [8/10], Loss 12084.2109, Entropy: 10723.2188, KL:  1360.9922 \n",
      "Epoch [9/10], Loss 12891.9844, Entropy: 11533.6133, KL:  1358.3712 \n",
      "Epoch [10/10], Loss 13302.7559, Entropy: 11883.7832, KL:  1418.9731 \n"
     ]
    }
   ],
   "source": [
    "# Model 5 - It3\n",
    "\n",
    "outputImages = []\n",
    "train(outputImages, 'DropTo10_IT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss 12427.7871, Entropy: 11069.5625, KL:  1358.2251 \n",
      "Epoch [2/30], Loss 12087.0352, Entropy: 10686.7334, KL:  1400.3016 \n",
      "Epoch [3/30], Loss 12625.4385, Entropy: 11244.0879, KL:  1381.3503 \n",
      "Epoch [4/30], Loss 12423.8730, Entropy: 11020.5430, KL:  1403.3303 \n",
      "Epoch [5/30], Loss 13073.8955, Entropy: 11644.2500, KL:  1429.6458 \n",
      "Epoch [6/30], Loss 13217.1074, Entropy: 11795.3018, KL:  1421.8062 \n",
      "Epoch [7/30], Loss 12549.2871, Entropy: 11161.4766, KL:  1387.8109 \n",
      "Epoch [8/30], Loss 13469.1992, Entropy: 12049.3623, KL:  1419.8367 \n",
      "Epoch [9/30], Loss 11737.7920, Entropy: 10300.0117, KL:  1437.7799 \n",
      "Epoch [10/30], Loss 12611.0391, Entropy: 11219.9668, KL:  1391.0725 \n",
      "Epoch [11/30], Loss 13148.8477, Entropy: 11735.4326, KL:  1413.4147 \n",
      "Epoch [12/30], Loss 12557.3145, Entropy: 11194.1709, KL:  1363.1440 \n",
      "Epoch [13/30], Loss 12220.1514, Entropy: 10825.6230, KL:  1394.5287 \n",
      "Epoch [14/30], Loss 12664.8535, Entropy: 11256.6191, KL:  1408.2349 \n",
      "Epoch [15/30], Loss 12764.9756, Entropy: 11320.1523, KL:  1444.8231 \n",
      "Epoch [16/30], Loss 12568.0029, Entropy: 11125.4004, KL:  1442.6023 \n",
      "Epoch [17/30], Loss 12839.1553, Entropy: 11494.1816, KL:  1344.9736 \n",
      "Epoch [18/30], Loss 12683.7578, Entropy: 11304.3652, KL:  1379.3921 \n",
      "Epoch [19/30], Loss 13008.9961, Entropy: 11537.9834, KL:  1471.0127 \n",
      "Epoch [20/30], Loss 12625.5664, Entropy: 11281.3682, KL:  1344.1984 \n",
      "Epoch [21/30], Loss 12158.5459, Entropy: 10743.8486, KL:  1414.6975 \n",
      "Epoch [22/30], Loss 12910.2588, Entropy: 11483.7500, KL:  1426.5088 \n",
      "Epoch [23/30], Loss 12210.4131, Entropy: 10793.2656, KL:  1417.1471 \n",
      "Epoch [24/30], Loss 12239.9336, Entropy: 10870.4033, KL:  1369.5308 \n",
      "Epoch [25/30], Loss 12874.7363, Entropy: 11474.3105, KL:  1400.4257 \n",
      "Epoch [26/30], Loss 13248.1494, Entropy: 11825.9600, KL:  1422.1898 \n",
      "Epoch [27/30], Loss 12560.2393, Entropy: 11152.1865, KL:  1408.0526 \n",
      "Epoch [28/30], Loss 12793.5361, Entropy: 11401.2461, KL:  1392.2897 \n",
      "Epoch [29/30], Loss 13262.0771, Entropy: 11890.8203, KL:  1371.2568 \n",
      "Epoch [30/30], Loss 12121.6729, Entropy: 10738.6797, KL:  1382.9929 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "# Model 5 - It4\n",
    "outputImages = []\n",
    "train(outputImages, 'DropTo10_IT4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
